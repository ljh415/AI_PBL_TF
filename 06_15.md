# 06/15 공유내용

# 학습환경

- CPU : AMD Ryzen 5800x
- GPU : RTX3060 (vram : 12G)
- ram : 32G

# Data

- 개, 고양이 이미지 모두 다 사용 (train 약 200만장)
- 다 읽어서 csv로 생성
    - bbox에서 에러가 있는 이미지들은 다 cleaning
        - 음수가 포함되어 있는 경우
        - 이미지의 높이 너비보다 큰 bbox element값이 나올경우

### 문제점

1. Data Imbalance
    
    ![Untitled](https://user-images.githubusercontent.com/48716219/173808845-94b47697-0b14-4c14-bb79-79f48bb07ffe.png)
    
    <aside>
    ✅ model.fit을 하면서 각 클래스에 대한 weight를 함께 학습 (class_weight)
    
    </aside>
    
    [불균형 데이터 분류 | TensorFlow Core](https://www.tensorflow.org/tutorials/structured_data/imbalanced_data?hl=ko#%ED%81%B4%EB%9E%98%EC%8A%A4_%EA%B0%80%EC%A4%91%EC%B9%98%EB%A1%9C_%EB%AA%A8%EB%8D%B8_%EA%B5%90%EC%9C%A1)
    
2. 너무 많은 데이터수
    
    200만장에 대해서 학습을 시키다보니 배치를 크게 잡기더라도 하드웨어적으로 한계가 있고, 학습이 한에폭에 약 4시간씩 걸렸음
    
    <aside>
    ✅ 랜덤 샘플링을 통해서 데이터를 반으로 줄여보았지만 학습이 제대로 진행되지 않아서 결국 다 사용
    ⇒ 유의미한 이미지들만 샘플링하기가 힘들다고 판단
    
    </aside>
    

# Model

- Feature Extraction
    - `EffiecientNetB0` (Input size : `224`)
- Classification
    - `GlobalAveragePooling2D`
        
        ⇒ local 적인 정보보다 데이터의 특성상, 개, 고양이의 **전체적인 몸의 형태**를 통해서 감정을 파악하는 것이기에, flatten보다 GAP를 사용
        
    - `BatchNormalization`
    - `Dense(activation=’softmax’)`

### 문제점

1. model의 가중치를 freeze 시키기
    
    200만장의 데이터를 모델 전체에 대해서 학습시키다보니 학습이 오래걸림.
    
    - Backbon 전체 freeze, classification부분만 학습
        
        ⇒ 학습은 빠르지만 val acc기준 1퍼센트도 간신히 나오는 정도로 제대로 분류해내지를 못함
        
    - Backbon의 마지막 bloch + Classification 부분만 학습
        
        ⇒ 전체를 다 학습시키는 것 보다는 빠르지만 위에서 시도한 것과 마찬가지로 큰 효과는 얻지 못함
        
    
    <aside>
    💡 결국 모델을 전체다 학습시키는 방향으로 선택
    
    </aside>
    

# Train

<aside>
💡 학습은 한 에폭에 약 4시간 소요
현재 최고 val acc는 `0.498`

</aside>

### Hyper parameter

- init lr : 5e-6
- input resolution : 224
- batch_size : 64

### callback

- Early Stopping
    - val loss기준, 5회 이상 개선이 되지 않는다면 학습 종료
- ModelCheckpoint
    - val acc기준, 높은 정확도의 모델을 저장

### 추후 개선 가능할 사항

- Learning rate Scheduler
- Warm-up + CosineAnealing
- `categorical_crossentropy`의 label smoothing 인자 활용